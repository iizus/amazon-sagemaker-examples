{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Bedrock Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "- Bedrockのbatch推論のquotaがon demandと関係ない（だからquotaに引っ掛からなくてうれしい）と言う説\n",
    "- text to image (SDXL or Titan or Both)で500枚くらいバッチ推論ジョブを発行してどんな風に実行が完了するか（もしくは完了しないか）を見てみる\n",
    "- On demandのquotaより明らかに早かったら嬉しい\n",
    "\n",
    "\n",
    "## 検証条件\n",
    "- 推論方式: バッチ\n",
    "- 生成: テキスト → 画像\n",
    "- モデル:\n",
    "    - amazon.titan-image-generator-v1\n",
    "    - stability.stable-diffusion-xl-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "### 入出力データ\n",
    "- インターフェイスはS3のみ。\n",
    "- バッチ推論は入出力ファイルサイズ制限がある。モデル間での差分はない。\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html#quotas-batch\n",
    "- 最小も最大も定義されている。\n",
    "\n",
    "#### 入力データフォーマット\n",
    "- 一枚一枚に対してはオンデマンド推論と同じ。それをJSONLにしてS3に置く。\n",
    "- リクエスト枚数が少ないと、ジョブ実行時の序盤にエラーで跳ね返される。\n",
    "    - Titan: 4枚\n",
    "    - SD: 10枚\n",
    "\n",
    "#### 出力データフォーマット\n",
    "それぞれのモデルで出力方法が全く異なるので注意\n",
    "\n",
    "- Titan：単一のJSONLにバイナリで全画像が入力情報とセットで埋まっている。大量画像入っているのでデカJSONL\n",
    "- SD：ファイル名に連番が入った一枚一枚のPNG\n",
    "\n",
    "入力と出力の突き合わせ\n",
    "\n",
    "- Titan：1行ずつの入力と出力がすぐ対応できるので、突き合わせしやすい\n",
    "- SD：連番で突き合わせ作業するしかない\n",
    "\n",
    "### ジョブ\n",
    "- ジョブは並列実行されない。\n",
    "- 複数のジョブを投げることは可能だが、順序保証されて（Undocumented）キューイングされて順次処理される。したがって前のジョブが完了するまで次のジョブは実行されない。\n",
    "- よってジョブの並列実行による高速化はされない。\n",
    "- またバッチ推論にすることでオンラインより高速化もされない。\n",
    "- バッチ推論ジョブの速度はオンデマンド推論を同期処理しているような速度のため、非同期でオンデマンド処理させた方が高速化は可能。\n",
    "\n",
    "### 結果\n",
    "600枚全て成功\n",
    "\n",
    "- Titan: 12 sec / image\n",
    "- SD: 7 sec /image　← 70分くらい\n",
    "\n",
    "条件全て揃えられているわけではないので注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公開情報\n",
    "- 開発者ドキュメント: https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference.html\n",
    "- Quota: https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html#quotas-batch\n",
    "- コードサンプル: https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-example.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境準備\n",
    "\n",
    "### 2024.2.1\n",
    "- Public Preview\n",
    "- 利用方法\n",
    "    - REST API: あると思うが面倒\n",
    "    - CLI: 無さそう\n",
    "    - SDK: プレビューのがある\n",
    "        - PythonとJavaのみ\n",
    "        - ただし他サービスと同様、プレビュー状態のAPIは一般公開SDKに含まれていない\n",
    "        - そのため以下の公式whlからインストールが必要\n",
    "        - https://d2eo22ngex1n9g.cloudfront.net/Documentation/SDK/bedrock-python-sdk-reinvent.zip\n",
    "    - コンソール: 無さそう\n",
    "\n",
    "Python SDKを使うこととし、以下でSDKのインストールをする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! chmod +x install_sdk.sh\n",
    "! ./install_sdk.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記実行後、カーネルの再起動が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import Application\n",
    "Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```aws s3 cp``` などをするため、AWS CLIがあるか確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! aws --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力データフォーマット\n",
    "\n",
    "参考: https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-data.html\n",
    "\n",
    "以下、サンプルの入力JSON\n",
    "\n",
    "入力JSON Linesフォーマット\n",
    "```JSON\n",
    "{\n",
    "    \"recordId\": \"12 character alphanumeric string\",\n",
    "    \"modelInput\": {JSON body}\n",
    "}\n",
    "...\n",
    "{\n",
    "    \"recordId\": \"12 character alphanumeric string\",\n",
    "    \"modelInput\": {JSON body}\n",
    "}\n",
    "```\n",
    "\n",
    "Titan imageの場合の推論入力JSON: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html#model-parameters-titan-image-api\n",
    "\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"inputText\": string,\n",
    "    \"textGenerationConfig\": {\n",
    "        \"temperature\": float,  \n",
    "        \"topP\": float,\n",
    "        \"maxTokenCount\": int,\n",
    "        \"stopSequences\": [string]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Tiatanの場合のバッチ推論入力JSON Lines\n",
    "```JSON\n",
    "{\n",
    "    \"recordId\": \"12 character alphanumeric string\",\n",
    "    \"modelInput\": {\n",
    "        \"inputText\": string,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"temperature\": float,  \n",
    "            \"topP\": float,\n",
    "            \"maxTokenCount\": int,\n",
    "            \"stopSequences\": [string]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "...\n",
    "{\n",
    "    \"recordId\": \"12 character alphanumeric string\",\n",
    "    \"modelInput\": {\n",
    "        \"inputText\": string,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"temperature\": float,  \n",
    "            \"topP\": float,\n",
    "            \"maxTokenCount\": int,\n",
    "            \"stopSequences\": [string]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共通条件設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size:int = 1024\n",
    "cfg_scale:int = 8.0\n",
    "seed:int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height:int = image_size\n",
    "width:int = image_size\n",
    "\n",
    "image_generation_config:dict = {\n",
    "    \"numberOfImages\": 1,\n",
    "    \"quality\": \"standard\",\n",
    "    \"height\": height,\n",
    "    \"width\": width,\n",
    "    \"cfgScale\": cfg_scale,\n",
    "    \"seed\": seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_body_for_claude(prompt:str) -> dict:\n",
    "    return {\n",
    "        \"prompt\": f\"\\n\\nHuman:{prompt}\\n\\nAssistant:\",\n",
    "        # \"temperature\": float,\n",
    "        # \"top_p\": float,\n",
    "        # \"top_k\": int,\n",
    "        # \"max_tokens_to_sample\": int,\n",
    "        # \"stop_sequences\": [string]\n",
    "    }\n",
    "\n",
    "def return_body_for_titan_image(prompt:str) -> dict:\n",
    "    return {\n",
    "        \"taskType\": \"TEXT_IMAGE\",\n",
    "        \"textToImageParams\": {\n",
    "            \"text\": prompt\n",
    "        },\n",
    "        \"imageGenerationConfig\": image_generation_config\n",
    "    }\n",
    "\n",
    "def return_body_for_sd(prompt:str) -> dict:\n",
    "    return {\n",
    "        \"text_prompts\": [{\"text\": prompt}],\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"cfg_scale\": cfg_scale,\n",
    "        # \"clip_guidance_preset\": string,\n",
    "        # \"sampler\": string,\n",
    "        # \"samples\",\n",
    "        \"seed\": seed,\n",
    "        # \"steps\": int,\n",
    "        # \"style_preset\": string,\n",
    "        # \"extras\": JSON object\n",
    "    }\n",
    "\n",
    "functions = {\n",
    "    'anthropic.claude-v2:1': return_body_for_claude,\n",
    "    'amazon.titan-image-generator-v1': return_body_for_titan_image,\n",
    "    'stability.stable-diffusion-xl-v1': return_body_for_sd,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件設定\n",
    "\n",
    "バッチ推論は入出力ファイルサイズ制限がある。モデル間での差分はない。\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html#quotas-batch\n",
    "\n",
    "最小も最大も定義されている。そのためリクエスト枚数が少ないと、ジョブ実行時の序盤にエラーで跳ね返される。\n",
    "\n",
    "- Titan: 4枚\n",
    "- SD: 10枚\n",
    "\n",
    "程度が最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ジョブ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bedrock import Bedrock\n",
    "bedrock:Bedrock = Bedrock(region=\"us-east-1\")\n",
    "\n",
    "from batch import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Bedrock' object has no attribute 'invoke_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/amazon-sagemaker-examples/bedrock/batch_inference/batch_inference.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wlb6pe4mdttlxgh.studio.us-east-1.sagemaker.aws/home/sagemaker-user/amazon-sagemaker-examples/bedrock/batch_inference/batch_inference.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m bedrock\u001b[39m.\u001b[39;49minvoke_model(\n\u001b[1;32m      <a href='vscode-notebook-cell://wlb6pe4mdttlxgh.studio.us-east-1.sagemaker.aws/home/sagemaker-user/amazon-sagemaker-examples/bedrock/batch_inference/batch_inference.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     body \u001b[39m=\u001b[39;49m return_body_for_claude(prompt\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m1 + 1 = ?\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wlb6pe4mdttlxgh.studio.us-east-1.sagemaker.aws/home/sagemaker-user/amazon-sagemaker-examples/bedrock/batch_inference/batch_inference.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model_id \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39manthropic.claude-v2:1\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wlb6pe4mdttlxgh.studio.us-east-1.sagemaker.aws/home/sagemaker-user/amazon-sagemaker-examples/bedrock/batch_inference/batch_inference.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m~/amazon-sagemaker-examples/bedrock/batch_inference/bedrock.py:16\u001b[0m, in \u001b[0;36mBedrock.invoke_model\u001b[0;34m(self, model_id, body)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke_model\u001b[39m(\u001b[39mself\u001b[39m, model_id:\u001b[39mstr\u001b[39m, body:\u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m json:\n\u001b[0;32m---> 16\u001b[0m     response:\u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49minvoke_model(\n\u001b[1;32m     17\u001b[0m         body \u001b[39m=\u001b[39m body,\n\u001b[1;32m     18\u001b[0m         modelId \u001b[39m=\u001b[39m model_id,\n\u001b[1;32m     19\u001b[0m         accept \u001b[39m=\u001b[39m Bedrock\u001b[39m.\u001b[39mcontent_type,\n\u001b[1;32m     20\u001b[0m         contentType \u001b[39m=\u001b[39m Bedrock\u001b[39m.\u001b[39mcontent_type,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:888\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m event_response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 888\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    889\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Bedrock' object has no attribute 'invoke_model'"
     ]
    }
   ],
   "source": [
    "bedrock.invoke_model(\n",
    "    body = return_body_for_claude(prompt='1 + 1 = ?'),\n",
    "    model_id = 'anthropic.claude-v2:1',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_prompts:int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = Batch(\n",
    "    model_id = \"amazon.titan-image-generator-v1\",\n",
    "    functions = functions,\n",
    "    bedrock = _bedrock,\n",
    ")\n",
    "ti_job_name, ti_output_dir = ti.create_inputs_by(\n",
    "    prompts = (f\"{i+2} dogs running at a park.\" for i in range(number_of_prompts))\n",
    ")\n",
    "! aws s3 cp s3://$Batch.bucket_name/$ti_output_dir$Batch.jsonl_file_name -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = Batch(\n",
    "    model_id = \"stability.stable-diffusion-xl-v1\",\n",
    "    functions = functions,\n",
    "    bedrock = _bedrock,\n",
    ")\n",
    "sd_job_name, sd_output_dir = sd.create_inputs_by(\n",
    "    prompts = (f\"{i+2} dogs running at a park.\" for i in range(number_of_prompts))\n",
    ")\n",
    "! aws s3 cp s3://$Batch.bucket_name/$sd_output_dir$Batch.jsonl_file_name -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.create_job(ti_job_name, ti_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.create_job(sd_job_name, sd_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all jobs: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status\n",
       "Completed    70\n",
       "Failed       28\n",
       "Stopped       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock.group_jobs_by_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大キュー数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "while True:\n",
    "    ti.create_job(ti_job_name, ti_output_dir)\n",
    "    sd.create_job(sd_job_name, sd_output_dir)\n",
    "    print(_bedrock.group_jobs_by_status())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude = Batch(\n",
    "    model_id = 'anthropic.claude-v2:1',\n",
    "    functions = functions,\n",
    "    bedrock = _bedrock.client,\n",
    ")\n",
    "claude_job_name, claude_output_dir = claude.create_inputs_by(\n",
    "    prompts = (f\"{i} + 1 = ?\" for i in range(number_of_prompts))\n",
    ")\n",
    "! aws s3 cp s3://$Batch.bucket_name/$claude_output_dir$Batch.jsonl_file_name -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude.create_job(claude_job_name, claude_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bedrock.group_jobs_by_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bedrock.get_dataframe_of_jobs(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude.wait()\n",
    "! aws s3 cp $claude_output_dir$claude.id/manifest.json.out -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジョブの状態はこちら: https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-list.html\n",
    "\n",
    "ジョブは並列実行されない。\n",
    "\n",
    "複数のジョブを投げることは可能だが、ジョブが順序保証されて（Undocumented）キューイングされて順次処理される。したがって前のジョブが完了するまで次のジョブは実行されない。\n",
    "\n",
    "よってジョブの並列実行による高速化はされない。\n",
    "\n",
    "またバッチ推論にすることでオンラインより高速化もされない。\n",
    "\n",
    "バッチ推論ジョブの速度はオンデマンド推論を同期処理しているような速度のため、非同期でオンデマンド処理させた方が高速化は可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像表示\n",
    "それぞれのモデルで出力方法が全く異なるので注意\n",
    "\n",
    "- Titan：単一のJSONLにbase64で全画像が入力情報とセットで埋まっている\n",
    "- SD：ファイル名に連番が入った一枚一枚のPNG\n",
    "\n",
    "入力と出力の突き合わせ\n",
    "\n",
    "- Titan：1行ずつの入力と出力がすぐ対応できるので、突き合わせしやすい\n",
    "- SD：連番で突き合わせ作業するしかない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64\n",
    "# from io import BytesIO\n",
    "# from PIL import Image\n",
    "\n",
    "# output_dir:str = f\"{dir}/{job_id}\"\n",
    "\n",
    "# def get_content_binary(key:str):\n",
    "#     output_obj = bucket.Object(key=output_key).get()\n",
    "#     binary_contents = output_obj.get(\"Body\").read()\n",
    "#     return binary_contents\n",
    "\n",
    "\n",
    "# def display_image(image_bytes):\n",
    "#     image = Image.open(BytesIO(image_bytes))\n",
    "#     image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# if \"titan\" in model_id:\n",
    "#     output_key = f\"{output_dir}/{model_id}.jsonl.out\"\n",
    "#     binary_contents = get_content_binary(output_key)\n",
    "\n",
    "#     for line in BytesIO(binary_contents):\n",
    "#         content = json.loads(line.decode(\"utf-8\"))\n",
    "#         # finish_reason = content.get(\"error\")\n",
    "#         # if finish_reason is not None: print(f\"Image generation error. Error is {finish_reason}\")\n",
    "#         print(content.get(\"modelInput\").get(\"textToImageParams\").get(\"text\"))\n",
    "#         base64_image = content.get(\"modelOutput\").get(\"images\")[0]\n",
    "#         base64_bytes = base64_image.encode('ascii')\n",
    "#         image_bytes = base64.b64decode(base64_bytes)\n",
    "#         display_image(image_bytes)\n",
    "# elif \"stable\" in model_id:\n",
    "#     for record_id in range(number_of_prompts):\n",
    "#         output_key = f\"{output_dir}/{model_id}.{str(record_id).zfill(12)}.1.png\"\n",
    "#         binary_contents = get_content_binary(output_key)\n",
    "#         print(output_key)\n",
    "#         display_image(binary_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "### 入出力データ\n",
    "- インターフェイスはS3のみ。\n",
    "- バッチ推論は入出力ファイルサイズ制限がある。モデル間での差分はない。\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/quotas.html#quotas-batch\n",
    "- 最小も最大も定義されている。\n",
    "\n",
    "#### 入力データフォーマット\n",
    "- 一枚一枚に対してはオンデマンド推論と同じ。それをJSONLにしてS3に置く。\n",
    "- リクエスト枚数が少ないと、ジョブ実行時の序盤にエラーで跳ね返される。\n",
    "    - Titan: 4枚\n",
    "    - SD: 10枚\n",
    "\n",
    "#### 出力データフォーマット\n",
    "それぞれのモデルで出力方法が全く異なるので注意\n",
    "\n",
    "- Titan：単一のJSONLにバイナリで全画像が入力情報とセットで埋まっている。大量画像入っているのでデカJSONL\n",
    "- SD：ファイル名に連番が入った一枚一枚のPNG\n",
    "\n",
    "入力と出力の突き合わせ\n",
    "\n",
    "- Titan：1行ずつの入力と出力がすぐ対応できるので、突き合わせしやすい\n",
    "- SD：連番で突き合わせ作業するしかない\n",
    "\n",
    "### ジョブ\n",
    "- ジョブは並列実行されない。\n",
    "- 複数のジョブを投げることは可能だが、順序保証されて（Undocumented）キューイングされて順次処理される。したがって前のジョブが完了するまで次のジョブは実行されない。\n",
    "- よってジョブの並列実行による高速化はされない。\n",
    "- またバッチ推論にすることでオンラインより高速化もされない。\n",
    "- バッチ推論ジョブの速度はオンデマンド推論を同期処理しているような速度のため、非同期でオンデマンド処理させた方が高速化は可能。\n",
    "\n",
    "### 結果\n",
    "600枚全て成功\n",
    "\n",
    "- Titan: 12 sec / image\n",
    "- SD: 7 sec /image　← 70分くらい\n",
    "\n",
    "条件全て揃えられているわけではないので注意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単発推論\n",
    "まずは単発を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titan image\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-image.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt:str = \"A dog running at a park.\"\n",
    "\n",
    "from func import generate_image_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"taskType\": \"TEXT_IMAGE\",\n",
    "    \"textToImageParams\": {\n",
    "        \"text\": prompt\n",
    "    },\n",
    "    \"imageGenerationConfig\": image_generation_config\n",
    "}\n",
    "generate_image_by(model_id=\"amazon.titan-image-generator-v1\", body=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Diffusion\n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-diffusion-1-0-text-image.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    \"text_prompts\": [{\"text\": prompt}],\n",
    "    \"height\": height,\n",
    "    \"width\": width,\n",
    "    \"cfg_scale\": cfg_scale,\n",
    "    # \"clip_guidance_preset\": string,\n",
    "    # \"sampler\": string,\n",
    "    # \"samples\",\n",
    "    \"seed\": seed,\n",
    "    # \"steps\": int,\n",
    "    # \"style_preset\": string,\n",
    "    # \"extras\" :JSON object\n",
    "}\n",
    "generate_image_by(model_id=\"stability.stable-diffusion-xl-v1\", body=body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
